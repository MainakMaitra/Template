#!/usr/bin/env python
# coding: utf-8

# In[1]:


get_ipython().run_line_magic('matplotlib', 'inline')
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder
from sklearn.base import TransformerMixin
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score


#Data Wrangling and Visualization

df = pd.read_csv("/home/tukai/Desktop/Hacker Rank Data Scientist Test/Predict  Life Expectancy/Train.txt", index_col = 0)
df.head()
df.shape
df.dtypes
df.describe()


# Discriptive statistic indicates that in general "surface_area" values (total area of lands) are smaller than "argicultural_land" and "forst_area" values (argicultural and forest portions of the total area). </n>It seems there is data entry problem. Following analysese are preformed to address this issue.

# Check if multi-collineerity is present 
sns.heatmap(df[['surface_area', 'agricultural_land', 'forest_area']].corr(), cmap='PRGn', annot=True)

sns.pairplot(df[['surface_area', 'agricultural_land', 'forest_area']])

# make a copy ofthe dataframe 
df1 = df.copy()

# Step 1: Multiply 'surface_area' by 100
df1['surface_area'] = df1['surface_area']*100

# Step 2: Creates new variables
df1['agricultural_portion'] = df1['agricultural_land'] / df1['surface_area'] 
df1['forest_portion'] = df1['forest_area'] / df1['surface_area']

# Step 3: Remove 'agricultural_land' and 'forest_area' colums from dataset
df1.drop(['agricultural_land', 'forest_area'], axis=1, inplace=True)

# Show the first 10 row of df1
df1.head(10)

sns.heatmap(df1[['surface_area', 'agricultural_portion', 'forest_portion']].corr(), cmap='PRGn', annot=True)

# Checking null values
df1.isnull().sum()

# To find for how many instances all the three values are missing
#len(df1[(df1['inflation_annual'].isnull()) & (df1['inflation_monthly'].isnull()) & (df1['inflation_weekly'].isnull())])

# Replace missing values in column 'inflation_annual' with information from
# columns 'inflation_monthly' & 'inflation_weekly'

for i in df1.index:
    if np.isnan(df1.loc[i, 'inflation_annual']):
        if np.isnan(df1.loc[i, 'inflation_monthly']):
            df1.loc[i, 'inflation_annual'] = 52 * df1.loc[i, 'inflation_weekly']
        else:
            df1.loc[i, 'inflation_annual'] = 12 * df1.loc[i, 'inflation_monthly']

# Remove columns 'inflation_monthly' & 'inflation_weekly'
df1.drop(['inflation_monthly', 'inflation_weekly'], axis=1, inplace=True)


# In[12]:


df1.isnull().sum()


# In[13]:


# Convert categorical and ordinal fearures into numeric features
# Decide which categorical variables you want to use in model
for col_name in df1.columns:
    if df1[col_name].dtypes == 'object':
        unique_cat = len(df1[col_name].unique())
        print("Feature '{col_name}' has {unique_cat} unique categories".format(col_name=col_name, unique_cat=unique_cat))


# <p><b>There are too many categories in 'internet_users'. It's more simillar to a numeric feature! Let's check...

# In[14]:


df1['internet_users']


# In[15]:


# Convert 'internet_users' column to associated percentages
variable_split = df1['internet_users'].str.split()
df1['percent_internet_users'] = (pd.to_numeric(variable_split.str.get(0), errors='coerce') / 
                          pd.to_numeric(variable_split.str.get(2), errors='coerce'))
# Remove 'internet_users' column
df1.drop('internet_users', axis=1, inplace=True)
df1.head()


# In[16]:


# Check the frequency of categories (labels) in each categorical variable 
for name in df1.select_dtypes(include=['object']):
    print(name,':')
    print(df1[name].value_counts(),'\n')


# In[17]:


# Assign "mobile_subscriptions" values to 1 if mobile subscriptions is
# less than 1 per person, otherwise 2:
df1['mobile_subscriptions'] = [1 if x == 'less than 1 per person' else 2 for x in df1['mobile_subscriptions']]


# In[18]:


# Assign "women_parliament_seats_rate" to 1 if women _parliament seat _rate is
# [0%-25%), 2 if it is [25%-75%), or 3 if unknown:

df1['women_parliament_seats_rate'] = (
    df1['women_parliament_seats_rate'].replace('[0%-25%)', 1))
df1['women_parliament_seats_rate'] = (
    df1['women_parliament_seats_rate'].replace('[25%-50%)', 2))
df1['women_parliament_seats_rate'] = (
    df1['women_parliament_seats_rate'].replace('[50%-75%)', 2))
df1['women_parliament_seats_rate'] = (
    df1['women_parliament_seats_rate'].replace('unknown', 3))


# In[19]:


# Assign numeric values to the levels of "national_income" (ordinal variable):
mapper_1 = {'very low': 1, 'medium low': 2, 'low': 3,
            'medium high': 4, 'high': 5, 'very high': 6,
            'unknown': 7}
# df1['national_income'].replace(mapper_1, inplace=True)
df1['national_income'] = df1['national_income'].map(mapper_1)


# <p><b>Handling missing data

# In[20]:


# make a copy of df1  
df2 = df1.copy()
# How much of your data is missing?
df2.isnull().sum().sort_values(ascending = False)


# In[21]:


# Impute missing values using Imputer in sklearn.preprocessing

class DataFrameImputer(TransformerMixin):

    def __init__(self):
        """Impute missing values.

        Columns of dtype object are imputed with the most frequent value 
        in column.

        Columns of other types are imputed with mean of column.

        """
    def fit(self, X, y=None):

        self.fill = pd.Series([X[c].value_counts().index[0]
            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],
            index=X.columns)

        return self

    def transform(self, X, y=None):
        return X.fillna(self.fill)
    
df2 = DataFrameImputer().fit_transform(df2)


# In[22]:


df2.isnull().sum().sort_values(ascending = False)


# In[23]:


from sklearn import preprocessing
le = preprocessing.LabelEncoder()
for column_name in df2.columns:
    if df2[column_name].dtype == object:
        df2[column_name] = le.fit_transform(df2[column_name])
    else:
        pass


# In[24]:


df2.head()


# <p><b>Visualization, Modeling, Machine Learning</b></p>
# Can you construct a reliable model that predicts the life expectancy of an area (country, region, group of countries) using socioeconomic variables and identify how different features influence their decision? Please explain your findings effectively to technical and non-technical audiences using comments and visualizations, if appropriate.
# 
# *Build an optimized model that effectively solves the business problem.
# *The model would be evaluated on the basis of Mean Absolute Error.
# *Read the Test.csv file and prepare features for testing.

# In[25]:


def plot_histogram(x):
    plt.hist(x, color='gray', edgecolor='black', alpha=0.8)
    plt.title("Histogram of '{var_name}'".format(var_name=x.name))
    plt.xlabel("Value")
    plt.ylabel("Frequency")
    plt.show()


# In[26]:


# Plot distribution of traget (outcome) variable in the training data
plot_histogram(df2['life_expectancy'])


# In[27]:


# Loading Test data
test_data=pd.read_csv('/home/tukai/Desktop/Hacker Rank Data Scientist Test/Predict  Life Expectancy/Test.txt',index_col=0)
test_data.head()


# In[28]:


# Create a copy of test dataframe
tdf = test_data.copy()

# Step 1: Multiply 'surface_area' by 100
tdf['surface_area'] = tdf['surface_area'] * 100

# Step 2: Creates new variables 
tdf['agricultural_portion'] = tdf['agricultural_land'] / tdf['surface_area'] 
tdf['forest_portion'] = tdf['forest_area'] / tdf['surface_area']

# Step 3: Remove 'agricultural_land' and 'forest_area' colums from dataset
tdf.drop(['agricultural_land', 'forest_area'], axis=1, inplace=True)

# Step 4: Replace missing values in column 'inflation_annual' with information from
# columns 'inflation_monthly' & 'inflation_weekly'

for i in tdf.index:
    if np.isnan(tdf.loc[i, 'inflation_annual']):
        if np.isnan(tdf.loc[i, 'inflation_monthly']):
            tdf.loc[i, 'inflation_annual'] = 52 * tdf.loc[i, 'inflation_weekly']
        else:
            tdf.loc[i, 'inflation_annual'] = 12 * tdf.loc[i, 'inflation_monthly']

# Step5: Remove columns 'inflation_monthly' & 'inflation_weekly'
tdf.drop(['inflation_monthly', 'inflation_weekly'], axis=1, inplace=True)

# Step6: Convert 'internet_users' column to associated percentages
variable_split = tdf['internet_users'].str.split()
tdf['percent_internet_users'] = (pd.to_numeric(variable_split.str.get(0), errors='coerce') / 
                          pd.to_numeric(variable_split.str.get(2), errors='coerce'))

# Step 7: Remove 'internet_users' column
tdf.drop('internet_users', axis=1, inplace=True)

# Step 8: Assign "mobile_subscriptions" values to 1 if mobile subscriptions is
# less than 1 per person, otherwise 2:
tdf['mobile_subscriptions'] = [1 if x == 'less than 1 per person' else 2 for x in tdf['mobile_subscriptions']]

# Step 9: Assign "women_parliament_seats_rate" to 1 if women _parliament seat _rate is
# [0%-25%), 2 if it is [25%-75%), or 3 if unknown:

tdf['women_parliament_seats_rate'] = (
    tdf['women_parliament_seats_rate'].replace('[0%-25%)', 1))
tdf['women_parliament_seats_rate'] = (
    tdf['women_parliament_seats_rate'].replace('[25%-50%)', 2))
tdf['women_parliament_seats_rate'] = (
    tdf['women_parliament_seats_rate'].replace('[50%-75%)', 2))
tdf['women_parliament_seats_rate'] = (
    tdf['women_parliament_seats_rate'].replace('unknown', 3))

# Step 10: Assign numeric values to the levels of "national_income" (ordinal variable):
mapper_1 = {'very low': 1, 'medium low': 2, 'low': 3,
            'medium high': 4, 'high': 5, 'very high': 6,
            'unknown': 7}
tdf['national_income'].replace(mapper_1, inplace=True)

# Step 11: Assign numeric values to the levels of "improved_sanitation" (Ordinal Variable):
mapper_2 = {'very low access': 1, 'low access': 2, 'medium access': 3,
            'high access': 4, 'very high access': 5, 'no info': 6}
tdf['improved_sanitation'].replace(mapper_2, inplace=True)

# Step12: Impute missing values using Imputer in sklearn.preprocessing
tdf = DataFrameImputer().fit_transform(tdf)
# Show the first 5 rows of tdf
tdf.head()


# <p><b>Random Forest Regression

# In[29]:


# Create outcome and input DataFrames
y = df2['life_expectancy'] 
X = df2.drop('life_expectancy', axis=1)
y.head()


# In[30]:


# Create train and validation datasets to build the Random Forest (RF) regression model and find the best set of the model parameters 
X_train, X_validation, y_train, y_validation= train_test_split(X, y,random_state = 0)


# In[31]:


# Use 'Grid Search' to find the best set of RF regression parameters 
# using full dataset with criterion = 'mean absolute error' (mae) and 
# random_state = 33

n_estimators = [50, 100, 150, 200]
max_features = [4, 8, 13]
max_depth = [5, 6, 7]
min_split = [2, 3, 4] 
min_leaf = [1, 2, 3]
best_score = 100

for n in n_estimators:
    for f in max_features:
        for d in max_depth:
            for s in min_split:   
                for l in min_leaf:
                     rf = RandomForestRegressor(
                     n_estimators = n, 
                     criterion = 'mae', 
                     max_features= f,
                     random_state = 33, 
                     oob_score = False,
                     max_depth = d, min_samples_split = s, 
                     min_samples_leaf = l)
            rf.fit (X, y)    
            y_model = rf.predict(X)
            #score = mean_absolute_error(y, y_model)
            score = - np.mean(cross_val_score(rf, X, y, cv=4, scoring = 'neg_mean_absolute_error'))
            if score <= best_score:
                best_score = score
                max_n = n
                max_f = f
                max_d = d
                max_s = s
                max_l = l


print ("Number of Estimators:", max_n)               
print ("Max features:", max_f)
print ("Max Depth:", max_d)
print ("Min Split:", max_s)
print ("Min Leaf:", max_l)
print("Best Mean Absolute Error: {:.3f}".format(best_score))


# In[32]:


# Fit a RF using best indentified parameters
rf = RandomForestRegressor(n_estimators=200, criterion = 'mae', max_features=8, random_state = 33,
                           max_depth=7, min_samples_split=4, min_samples_leaf=3)

rf.fit(X, y)


# In[33]:


# Pridict test instances using test dataframe (tdf)
y_test = rf.predict(tdf)
y_test


# In[34]:


# Plot histograms to compare distribution of actual outcomes vs. prediction 
def plot_histogram_comp(x,y):
    plt.hist(x, alpha=0.5, edgecolor='black', label='Actual')
    plt.hist(y, alpha=0.5, edgecolor='black', label='Prediction')
    plt.title("Histogram of actual outcomes v.s predicted outcomes")
    plt.xlabel("Value")
    plt.ylabel("Frequency")
    plt.legend(loc='upper left')
    plt.show()


# In[35]:


# Check to see if distribution of actual target values is close to the distribution
# of predicted target values
plot_histogram_comp(y,y_test)


# <p><b> The government wants to know what are the most important features for your model. Can you tell them? <br> </b> Visualize the top 13 features and their feature importance.

# In[ ]:





# In[38]:


# Extract feature importance determined by RF model
feature_imp = pd.Series(rf.feature_importances_, index=X.columns)
feature_imp.sort_values(ascending=True, inplace=True)

# Creating a bar plot
feature_imp.plot(kind='barh', width=0.8, figsize=(7,7));


# Task:
# Submit the predictions on the test dataset using your optimized model
# For each record in the test set (Test.csv), you must predict the value of the life_expectancy variable. You should submit a CSV file with a header row and one row per test entry. The file (submissions.csv) should have exactly 2 columns:
# The file (submissions.csv) should have exactly 2 columns:
# 
# * id
# * life_expectancy

# In[39]:


# Create a submission_df
d = {'id': test_data.index, 'life_expectancy': y_test}
submission_df = pd.DataFrame(data=d)
submission_df


# In[40]:


#Submission
submission_df.to_csv('submissions.csv',index=False)


# In[ ]:




